{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, r\"C:\\Users\\wujingyu\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\")\n",
    "\n",
    "from your_code import HingeLoss, SquaredLoss\n",
    "from your_code import metrics\n",
    "from your_code import GradientDescent, load_data\n",
    "from your_code import L1Regularization, L2Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentQ1:\n",
    "    \"\"\"\n",
    "    This is a linear classifier similar to the one you implemented in the\n",
    "    linear regressor homework. This is the classification via regression\n",
    "    case. The goal here is to learn some hyperplane, y = w^T x + b, such that\n",
    "    when features, x, are processed by our model (w and b), the result is\n",
    "    some value y. If y is in [0.0, +inf), the predicted classification label\n",
    "    is +1 and if y is in (-inf, 0.0) the predicted classification label is\n",
    "    -1.\n",
    "\n",
    "    The catch here is that we will not be using the closed form solution,\n",
    "    rather, we will be using gradient descent. In your fit function you\n",
    "    will determine a loss and update your model (w and b) using gradient\n",
    "    descent. More details below.\n",
    "\n",
    "    Arguments:\n",
    "        loss - (string) The loss function to use. Either 'hinge' or 'squared'.\n",
    "        regularization - (string or None) The type of regularization to use.\n",
    "            One of 'l1', 'l2', or None. See regularization.py for more details.\n",
    "        learning_rate - (float) The size of each gradient descent update step.\n",
    "        reg_param - (float) The hyperparameter that controls the amount of\n",
    "            regularization to perform. Must be non-negative.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss, regularization=None,\n",
    "                 learning_rate=0.01, reg_param=0.05, question='1a'):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Select regularizer\n",
    "        if regularization == 'l1':\n",
    "            regularizer = L1Regularization(reg_param)\n",
    "        elif regularization == 'l2':\n",
    "            regularizer = L2Regularization(reg_param)\n",
    "        elif regularization is None:\n",
    "            regularizer = None\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Regularizer {} is not defined'.format(regularization))\n",
    "\n",
    "        # Select loss function\n",
    "        if loss == 'hinge':\n",
    "            self.loss = HingeLoss(regularizer)\n",
    "        elif loss == 'squared':\n",
    "            self.loss = SquaredLoss(regularizer)\n",
    "        else:\n",
    "            raise ValueError('Loss function {} is not defined'.format(loss))\n",
    "\n",
    "        self.model = None\n",
    "        self.question = question\n",
    "\n",
    "    def fit(self, features, targets, batch_size=None, max_iter=1000):\n",
    "        \"\"\"\n",
    "        Fits a gradient descent learner to the features and targets. The\n",
    "        pseudocode for the fitting algorithm is as follow:\n",
    "          - Initialize the model parameters to uniform random values in the\n",
    "            interval [-0.1, +0.1].\n",
    "          - While not converged:\n",
    "            - Compute the gradient of the loss with respect to the current\n",
    "              batch.\n",
    "            - Update the model parameters by moving them in the direction\n",
    "              opposite to the current gradient. Use the learning rate as the\n",
    "              step size.\n",
    "        For the convergence criteria, compute the loss over all examples. If\n",
    "        this loss changes by less than 1e-4 during an update, assume that the\n",
    "        model has converged. If this convergence criteria has not been met\n",
    "        after max_iter iterations, also assume convergence and terminate.\n",
    "\n",
    "        You should include a bias term by APPENDING a column of 1s to your\n",
    "        feature matrix. The bias term is then the last value in self.model.\n",
    "\n",
    "        Arguments:\n",
    "            features - (np.array) An Nxd array of features, where N is the\n",
    "                number of examples and d is the number of features.\n",
    "            targets - (np.array) A 1D array of targets of length N.\n",
    "            batch_size - (int or None) The number of examples used in each\n",
    "                iteration. If None, use all of the examples in each update.\n",
    "            max_iter - (int) The maximum number of updates to perform.\n",
    "        Modifies:\n",
    "            self.model - (np.array) A 1D array of model parameters of length\n",
    "                d+1. The +1 refers to the bias term.\n",
    "        \"\"\"\n",
    "        accuracy_list = []\n",
    "        loss_list = []\n",
    "        iteration_list = []\n",
    "        self.model = np.zeros(features[0].shape)\n",
    "        for i in range(len(self.model)):\n",
    "            self.model[i] = random.uniform(-0.1, 0.1)\n",
    "        iteration = 0\n",
    "        loss = None\n",
    "        while iteration < max_iter:\n",
    "            if batch_size is None:\n",
    "                sample_features = features\n",
    "                sample_targets = targets\n",
    "            else:\n",
    "                sample_features = random.sample(features, batch_size)\n",
    "                sample_targets = random.sample(targets, batch_size)\n",
    "\n",
    "            self.model = self.model - self.learning_rate * \\\n",
    "                self.loss.backward(sample_features, self.model, sample_targets)\n",
    "            new_loss = self.loss.forward(\n",
    "                sample_features, self.model, sample_targets)\n",
    "            if loss is not None and abs(new_loss - loss) < 1e-4:\n",
    "                break\n",
    "            loss = new_loss\n",
    "            loss_list.append(loss)\n",
    "            accuracy = metrics.accuracy(targets, self.predict(features))\n",
    "            accuracy_list.append(accuracy)\n",
    "            iteration_list.append(iteration)\n",
    "            iteration += 1\n",
    "        plt.figure()\n",
    "        plt.plot(iteration_list, loss_list, color='orange', label='Loss')\n",
    "        plt.plot(iteration_list, accuracy_list, color='blue', label='Accuracy')\n",
    "        plt.title('Loss & Accuracy Vs. Iteration No.')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss & Accuracy')\n",
    "        plt.legend(loc=\"best\")\n",
    "        if self.question == '1a':\n",
    "            plt.savefig(\"Q1a.png\")\n",
    "        if self.question == '1b':\n",
    "            plt.savefig(\"Q1b.png\")\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Predicts the class labels of each example in features. Model output\n",
    "        values at and above 0 are predicted to have label +1. Non-positive\n",
    "        output values are predicted to have label -1.\n",
    "\n",
    "        NOTE: your predict function should make use of your confidence\n",
    "        function (see below).\n",
    "\n",
    "        Arguments:\n",
    "            features - (np.array) A Nxd array of features, where N is the\n",
    "                number of examples and d is the number of features.\n",
    "        Returns:\n",
    "            predictions - (np.array) A 1D array of predictions of length N,\n",
    "                where index d corresponds to the prediction of row N of\n",
    "                features.\n",
    "        \"\"\"\n",
    "        confidence = self.confidence(features)\n",
    "        predictions = np.zeros(confidence.shape)\n",
    "        for i in range(len(confidence)):\n",
    "            predictions[i] = np.sign(confidence[i])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def confidence(self, features):\n",
    "        \"\"\"\n",
    "        Returns the raw model output of the prediction. In other words, rather\n",
    "        than predicting +1 for values above 0 and -1 for other values, this\n",
    "        function returns the original, unquantized value.\n",
    "\n",
    "        Arguments:\n",
    "            features - (np.array) A Nxd array of features, where N is the\n",
    "                number of examples and d is the number of features.\n",
    "        Returns:\n",
    "            confidence - (np.array) A 1D array of confidence values of length\n",
    "                N, where index d corresponds to the confidence of row N of\n",
    "                features.\n",
    "        \"\"\"\n",
    "        confidence = features.dot(self.model)\n",
    "        return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1a\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\train-labels-idx1-ubyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wujingyu\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\\experiments\\FreeResponseQuestionsQ1.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000003?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhinge\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000003?line=7'>8</a>\u001b[0m regularization \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000003?line=9'>10</a>\u001b[0m train_features, test_features, train_targets, test_targets \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000003?line=10'>11</a>\u001b[0m         load_data(\u001b[39m'\u001b[39;49m\u001b[39mmnist-binary\u001b[39;49m\u001b[39m'\u001b[39;49m, fraction\u001b[39m=\u001b[39;49mfraction)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000003?line=12'>13</a>\u001b[0m \u001b[39m# train_features, test_features, train_targets, test_targets = \\\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000003?line=13'>14</a>\u001b[0m \u001b[39m#     load_data('mnist-binary', fraction=fraction)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000003?line=14'>15</a>\u001b[0m learner \u001b[39m=\u001b[39m GradientDescentQ1(loss\u001b[39m=\u001b[39mloss, regularization\u001b[39m=\u001b[39mregularization,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000003?line=15'>16</a>\u001b[0m                             learning_rate\u001b[39m=\u001b[39mlearning_rate, reg_param\u001b[39m=\u001b[39mreg_param, question\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m1a\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\\your_code\\load_data.py:42\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(dataset, fraction, base_folder)\u001b[0m\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=37'>38</a>\u001b[0m     train_features, test_features, train_targets, test_targets \u001b[39m=\u001b[39m \\\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=38'>39</a>\u001b[0m         load_json_data(path)\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=39'>40</a>\u001b[0m \u001b[39melif\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmnist-binary\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=40'>41</a>\u001b[0m     train_features, test_features, train_targets, test_targets \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=41'>42</a>\u001b[0m         load_mnist_data(\u001b[39m2\u001b[39;49m, fraction\u001b[39m=\u001b[39;49mfraction, mnist_folder\u001b[39m=\u001b[39;49mbase_folder)\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=42'>43</a>\u001b[0m     train_targets \u001b[39m=\u001b[39m train_targets \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=43'>44</a>\u001b[0m     test_targets \u001b[39m=\u001b[39m test_targets \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\\your_code\\load_data.py:126\u001b[0m, in \u001b[0;36mload_mnist_data\u001b[1;34m(threshold, fraction, examples_per_class, mnist_folder)\u001b[0m\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=123'>124</a>\u001b[0m     train_features, train_targets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[]]), np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=124'>125</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=125'>126</a>\u001b[0m     train_features, train_targets \u001b[39m=\u001b[39m _load_mnist(\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=126'>127</a>\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m, digits\u001b[39m=\u001b[39;49m\u001b[39mrange\u001b[39;49m(threshold), path\u001b[39m=\u001b[39;49mmnist_folder)\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=127'>128</a>\u001b[0m     train_features, train_targets \u001b[39m=\u001b[39m stratified_subset(\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=128'>129</a>\u001b[0m         train_features, train_targets, train_examples)\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=129'>130</a>\u001b[0m     train_features \u001b[39m=\u001b[39m train_features\u001b[39m.\u001b[39mreshape((\u001b[39mlen\u001b[39m(train_features), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32m~\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\\your_code\\load_data.py:208\u001b[0m, in \u001b[0;36m_load_mnist\u001b[1;34m(path, dataset, digits, asbytes, selection, return_labels, return_indices)\u001b[0m\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=204'>205</a>\u001b[0m \u001b[39m# We can skip the labels file only if digits aren't specified and labels\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=205'>206</a>\u001b[0m \u001b[39m# aren't asked for\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=206'>207</a>\u001b[0m \u001b[39mif\u001b[39;00m return_labels \u001b[39mor\u001b[39;00m digits \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=207'>208</a>\u001b[0m     flbl \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(labels_fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=208'>209</a>\u001b[0m     magic_nr, size \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m>II\u001b[39m\u001b[39m\"\u001b[39m, flbl\u001b[39m.\u001b[39mread(\u001b[39m8\u001b[39m))\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=209'>210</a>\u001b[0m     labels_raw \u001b[39m=\u001b[39m pyarray(\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, flbl\u001b[39m.\u001b[39mread())\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\train-labels-idx1-ubyte'"
     ]
    }
   ],
   "source": [
    "print('Question 1a')\n",
    "max_iter = 1000\n",
    "batch_size = None\n",
    "fraction = 1\n",
    "learning_rate = 1e-4\n",
    "reg_param = 0.05\n",
    "loss = 'hinge'\n",
    "regularization = None\n",
    "\n",
    "train_features, test_features, train_targets, test_targets = \\\n",
    "        load_data('mnist-binary', fraction=fraction)\n",
    "\n",
    "# train_features, test_features, train_targets, test_targets = \\\n",
    "#     load_data('mnist-binary', fraction=fraction)\n",
    "learner = GradientDescentQ1(loss=loss, regularization=regularization,\n",
    "                            learning_rate=learning_rate, reg_param=reg_param, question='1a')\n",
    "learner.fit(train_features, train_targets)\n",
    "predictions = learner.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(labels_fname, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\train-labels-idx1-ubyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wujingyu\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\\experiments\\FreeResponseQuestionsQ1.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39myour_code\u001b[39;00m \u001b[39mimport\u001b[39;00m GradientDescent\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000005?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000005?line=4'>5</a>\u001b[0m trainF, testF, trainT, testT \u001b[39m=\u001b[39m load_data(\u001b[39m'\u001b[39;49m\u001b[39mmnist-binary\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000005?line=5'>6</a>\u001b[0m learner \u001b[39m=\u001b[39m GradientDescent(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhinge\u001b[39m\u001b[39m'\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wujingyu/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/experiments/FreeResponseQuestionsQ1.ipynb#ch0000005?line=6'>7</a>\u001b[0m learner\u001b[39m.\u001b[39mfit(trainF, trainT, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\\your_code\\load_data.py:42\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(dataset, fraction, base_folder)\u001b[0m\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=37'>38</a>\u001b[0m     train_features, test_features, train_targets, test_targets \u001b[39m=\u001b[39m \\\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=38'>39</a>\u001b[0m         load_json_data(path)\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=39'>40</a>\u001b[0m \u001b[39melif\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmnist-binary\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=40'>41</a>\u001b[0m     train_features, test_features, train_targets, test_targets \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=41'>42</a>\u001b[0m         load_mnist_data(\u001b[39m2\u001b[39;49m, fraction\u001b[39m=\u001b[39;49mfraction, mnist_folder\u001b[39m=\u001b[39;49mbase_folder)\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=42'>43</a>\u001b[0m     train_targets \u001b[39m=\u001b[39m train_targets \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=43'>44</a>\u001b[0m     test_targets \u001b[39m=\u001b[39m test_targets \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\\your_code\\load_data.py:126\u001b[0m, in \u001b[0;36mload_mnist_data\u001b[1;34m(threshold, fraction, examples_per_class, mnist_folder)\u001b[0m\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=123'>124</a>\u001b[0m     train_features, train_targets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[]]), np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=124'>125</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=125'>126</a>\u001b[0m     train_features, train_targets \u001b[39m=\u001b[39m _load_mnist(\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=126'>127</a>\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m, digits\u001b[39m=\u001b[39;49m\u001b[39mrange\u001b[39;49m(threshold), path\u001b[39m=\u001b[39;49mmnist_folder)\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=127'>128</a>\u001b[0m     train_features, train_targets \u001b[39m=\u001b[39m stratified_subset(\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=128'>129</a>\u001b[0m         train_features, train_targets, train_examples)\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=129'>130</a>\u001b[0m     train_features \u001b[39m=\u001b[39m train_features\u001b[39m.\u001b[39mreshape((\u001b[39mlen\u001b[39m(train_features), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32m~\\OneDrive - HKUST Connect\\Documents\\交换\\Northwestern University\\year3_spring\\CS_349\\hw\\winter2022-hw5-gradient-descent-ElaineWu66\\your_code\\load_data.py:208\u001b[0m, in \u001b[0;36m_load_mnist\u001b[1;34m(path, dataset, digits, asbytes, selection, return_labels, return_indices)\u001b[0m\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=204'>205</a>\u001b[0m \u001b[39m# We can skip the labels file only if digits aren't specified and labels\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=205'>206</a>\u001b[0m \u001b[39m# aren't asked for\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=206'>207</a>\u001b[0m \u001b[39mif\u001b[39;00m return_labels \u001b[39mor\u001b[39;00m digits \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=207'>208</a>\u001b[0m     flbl \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(labels_fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=208'>209</a>\u001b[0m     magic_nr, size \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m>II\u001b[39m\u001b[39m\"\u001b[39m, flbl\u001b[39m.\u001b[39mread(\u001b[39m8\u001b[39m))\n\u001b[0;32m    <a href='file:///~/OneDrive%20-%20HKUST%20Connect/Documents/%E4%BA%A4%E6%8D%A2/Northwestern%20University/year3_spring/CS_349/hw/winter2022-hw5-gradient-descent-ElaineWu66/your_code/load_data.py?line=209'>210</a>\u001b[0m     labels_raw \u001b[39m=\u001b[39m pyarray(\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, flbl\u001b[39m.\u001b[39mread())\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\train-labels-idx1-ubyte'"
     ]
    }
   ],
   "source": [
    "from your_code import load_data\n",
    "from your_code import GradientDescent\n",
    "import numpy as np\n",
    "\n",
    "trainF, testF, trainT, testT = load_data('mnist-binary')\n",
    "learner = GradientDescent(loss='hinge', learning_rate=1e-4)\n",
    "learner.fit(trainF, trainT, batch_size=1, max_iter=1000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d026c32baa0f03cdb8ad6fe7239067c1356fdce055f41c5fb499fc4e045eb4b3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('hw5-gradient-descent')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
